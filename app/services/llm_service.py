# Import necessary types and Utilities

import ollama
from typing import List, Dict, Any
from app.utils.config import settings
from app.utils.logger import logger


class OllamaService:
    def __init__(self):
        """
        Initialize the OllamaService instance.

        - Create an Ollama client configured with the base URL from the app settings.
        - Set the model name to be used for all requests from the app settings.
        
        This setup allows all subsequent calls to interact with the Ollama LLM API.
        """
        self.client = ollama.Client(host=settings.ollama_base_url)
        self.model = settings.ollama_model

    async def generate_response(self, prompt: str, context: str = "") -> str:
        """
        Generate a text response from the Ollama LLM based on the user's prompt.
        
        Args:
            prompt (str): The main user query or input message.
            context (str, optional): Additional conversation history or context to guide the response.
        
        Returns:
            str: The generated response text from the model.
        
        Workflow:
        - If context is provided, combine it with the prompt, formatting to keep clarity.
        - Use the Ollama client's chat method to send a message with the combined prompt.
        - Extract and return the content of the model's reply.
        
        Error Handling:
        - Log any exceptions encountered during API calls.
        - Return a polite error message if something goes wrong.
        
        Note:
        - The method is asynchronous to fit into async web frameworks like FastAPI.
        - Ollama client calls are currently synchronous; consider async wrappers if needed.
        """
        try:
            # Prepare the complete prompt by combining context with user input if context exists
            full_prompt = f"{context}\n\nUser Query: {prompt}" if context else prompt

            # Call Ollama's chat API with the model and user message
            response = self.client.chat(
                model=self.model,
                messages=[
                    {"role": "user", "content": full_prompt}
                ]
            )

            # Return the response content generated by the model
            return response['message']['content']
        except Exception as e:
            # Log error details for debugging
            logger.error(f"Error generating response: {e}")

            # Return a fallback message to the user
            return "I apologize, but I'm having trouble processing your request right now."

    async def generate_embedding(self, text: str) -> List[float]:
        """
        Generate a vector embedding for a given text input using the Ollama model.
        
        Args:
            text (str): The input text to convert into a numeric vector representation.
        
        Returns:
            List[float]: A list of floats representing the embedding vector.
        
        Purpose:
        - Embeddings are useful for semantic search, similarity matching, or clustering.
        
        Process:
        - Send the input text to Ollama's embeddings API endpoint.
        - Extract the embedding vector from the response.
        
        Error Handling:
        - Log any exceptions during the API call.
        - Return an empty list if embedding generation fails.
        """
        try:
            # Request embedding vector from Ollama model
            response = self.client.embeddings(
                model=self.model,
                prompt=text
            )
            # Return the embedding array from the response
            return response['embedding']
        except Exception as e:
            # Log the error for troubleshooting
            logger.error(f"Error generating embedding: {e}")

            # Return empty embedding on failure
            return []

    async def classify_intent(self, message: str) -> Dict[str, Any]:
        """
        Classify the intent of a user message into predefined categories using the LLM.

        Args:
            message (str): The user's message to classify.
        
        Returns:
            Dict[str, Any]: Dictionary containing:
                - 'category' (str): The predicted intent category.
                - 'confidence' (float): The confidence score (0 to 1).
        
        Workflow:
        - Construct a prompt instructing the model to classify the message.
        - Use the generate_response method to get the LLM's classification.
        - Parse the response, expecting the format: CATEGORY|CONFIDENCE.
        - If parsing fails, default to 'GENERAL' with 0.5 confidence.
        
        Categories:
        - IT_HARDWARE: Issues with physical devices like printers, computers.
        - IT_SOFTWARE: Application or system software problems.
        - HR: Human resources related queries.
        - ACCOUNTING: Finance and billing related queries.
        - GENERAL: Any other/general inquiries.

        This helps in routing messages appropriately in a helpdesk scenario.
        """
        # Multi-line prompt to guide the LLM on classification task with instructions
        prompt = f"""
        Classify the following user message into one of these categories:
        - IT_HARDWARE: Hardware issues, computer problems, printer issues
        - IT_SOFTWARE: Software problems, application errors, system issues
        - HR: Human resources, payroll, benefits, policies
        - ACCOUNTING: Finance, expenses, billing, invoicing
        - GENERAL: General inquiries, other topics

        Message: "{message}"

        Respond with only the category name and confidence (0-1):
        Format: CATEGORY|CONFIDENCE
        """

        # Call the LLM to get the classification response
        response = await self.generate_response(prompt)

        try:
            # Parse the LLM response by splitting on '|'
            parts = response.strip().split('|')

            # Extract category and confidence (confidence defaults to 0.8 if missing)
            category = parts[0].strip()
            confidence = float(parts[1].strip()) if len(parts) > 1 else 0.8

            # Return structured classification result
            return {"category": category, "confidence": confidence}
        except:
            # In case of any parsing errors, fallback to GENERAL category with medium confidence
            return {"category": "GENERAL", "confidence": 0.5}


# Create a singleton instance of the OllamaService for reuse across the app
llm_service = OllamaService()
